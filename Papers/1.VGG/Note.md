# VGG

## 1. 研究背景

**卷积神经网络**在**大规模图像和视频识别领域**发展迅猛

* 大规模公开数据集（ImageNet， CIFAR等）
* 高性能计算系统的应用

---

## 2. 研究空白

**网络深度**对性能的影响在此前尚未有具体研究

---

## 3. 模型架构设计

作者设计了五种不同的网络配置，以A~E标记

<div align="center">
    <img src="image/1.jpg" alt="table1" width="500">
<div\>

### 3.1 输入

224x224的RGB图像，减去RGB均值

#### 预处理减去RGB均值的作用：

* 数据中心化
* 消除光照偏差
* 加速收敛

目前先进方法会使用**其他正则化方法**

### 3.2 卷积层

作者设计了两种卷积核，3x3和1x1 (1x1卷积核只在配置C中使用)

* 步长为**1**
* 3x3卷积核的周边填充为1像素，以保持卷积后分辨率不变

### 3.3 池化层

* 5个max-pooling层跟在某些卷积层后面（不是所有卷积层都跟pooling）
* Max-pooling在2×2像素窗口上执行，步长为2

### 3.4 全连接层
卷积层堆叠后跟随3个全连接层，前两个各有4096个通道，后一个有1000个通道，执行1000路ILSVRC分类，对应ImageNet数据集的1000个类别，最后通过一个softmax层输出概率。

### 3.5 激活函数
所有隐藏层间的连接使用ReLU激活函数：

$$
\mathrm{ReLU}(x) = \max(0, x)
$$

### 3.6 归一化
在**A-LRN**配置中，第一个卷积层后使用了LRN归一化，不过实验数据表明额外的归一化对结果没有提升，且会占用更多计算资源，因此其余配置没有再使用。

>局部响应归一化（Local Response Normalization, LRN）是一种在早期卷积神经网络中常用的归一化方法，旨在增强模型的泛化能力和抑制过拟合。其思想是模拟生物神经网络中的“侧抑制”机制，对相邻通道间的活动进行归一化：
>
>$$
>b_{x,y}^{i} = a_{x,y}^{i} \Big/ \left( k + \alpha \sum\limits_{j=\max(0,\,i-n/2)}^{\min(N-1,\,i+n/> 2)} (a_{x,y}^{j})^2 \right)^{\beta}
>$$
>
>其中：
>- $a_{x,y}^{i}$ 表示第 $i$ 个通道在位置 $(x, y)$ 的输入激活值；
>- $b_{x,y}^{i}$ 表示归一化后的结果；
>- $n$ 是归一化的相邻通道数；
>- $N$ 是总通道数；
>- $k, \alpha, \beta$ 是超参数，常用 $k=2$, $\alpha=10^{-4}$, $\beta=0.75$。
>
>LRN 通过让大的响应在归一化区域内抑制其他激活，有助于模型学习更加稀疏和有判别性的特征。

### 3.6 结构讨论

#### 与前人工作的主要区别

**传统方法（ILSVRC-2012/2013）：**
* Krizhevsky et al. (2012): 第一层使用11×11卷积，步长4
* Zeiler & Fergus (2013): 第一层使用7×7卷积，步长2

**VGG的创新：**
* 全程使用3×3小卷积核
* 步长为**1**，在每个像素上进行卷积

#### 为什么堆叠小卷积核更好？

**1. 感受野等效性**

* 2个3×3卷积层的等效感受野为5×5
* 3个3×3卷积层的等效感受野为7×7

> **感受野（Receptive Field）**：神经网络中某个神经元在输入空间（如原始图像）上所对应的区域大小。即：该神经元输出的计算会受输入中特定区域的像素影响。在卷积神经网络中，随着卷积层和池化层的堆叠，深层神经元的感受野会逐渐扩大，能够“看到”更大范围的输入内容，从而捕捉更高级、更全局的特征。

**2. 更多非线性（更强的判别能力）**

使用多个小核卷积层，相较于一个大核卷积层，可以引入**更多的非线性激活函数**，使决策函数更具判别性

**3. 参数量大幅减少**

假设输入和输出都有C个通道：
* 3个3×3层：参数量 = 3×(3²×C²) = 27C²
* 1个7×7层：参数量 = 7²×C² = 49C²
* 节省比例：(49-27)/49 ≈ **44.9%的参数减少**

**4. 隐式正则化**

模型设计可视为对7×7卷积滤波器施加正则化，强制其通过3×3滤波器分解

### 3.7 针对配置C (最常用模型) 的1x1卷积核的作用

在不影响卷积层感受野的情况下增加决策函数的非线性，源于"Network in Network"架构（Lin et al., 2014）

---